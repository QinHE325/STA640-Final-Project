---
title: "Does sample splitting help when estimating causal effects using double machine learning methods?"
author: "TJ Tang, Qin He"
date: "`r Sys.Date()`"
output: word_document
---

```{r setup, include=FALSE}
library(cowplot)
library(magick)
library(ggplot2)
library(flextable)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(dpi = 300) 
options(digits = 4)
```

## 1. Introduction

Athey and Imbens (2016) proposed an “honest” approach to estimate causal effects using classification and regression trees (CART) that separate samples used for constructing the partition and estimating effects within leaves of the partition. Their study shows that honest estimation can remove the bias and improve in the coverage of confidence intervals. The intuition behind the honest criterion is to avoid using same data or information twice for different tasks, more specifically, to select a model structure and to make estimation given a mode structure (Athey and Imbens 2016). Sample splitting follows the honesty criterion and is indeed one of the key issues when adapting machine learning methods to causal inference problems. However, some anecdotes saying that sample splitting will not help in finite samples. Therefore, this paper study will first go through Chernozhukov et al. (2018) especially about an example that sample splitting reduces the bias, and then will conduct a few simulation studies to figure out the effect of sample splitting when estimating causal effects. 

## 2. Double Machine Learning and Sample Splitting

Chernozhukov et al. (2018) establish that in very high dimension cases, machine learning methods will employ regularization to reduce variance but cause the regularization bias and overfitting on the parameters of interest or causal treatment effect parameter, for example average treatment effect. They show that the bias and overfit can be removed by double machine learning methods with sample splitting and cross fitting. Applying double machine learning methods can overcome the regularization bias because it employs an orthogonalized formulation. Meanwhile, sample splitting and cross fitting can reduce the bias due to the overfitting because we can avoid using the same data twice, to train the estimator and predict the missing potential outcomes. The details for orthogonalization and why double machine learning methods reduce the regularization bias is not the focus of this project. Instead, this project will focus more on the sample splitting and the effect to reduce overfitting bias.  

Wager and Athey (2018) illustrated double-sample trees as a motivation to reduce bias. They divided training subsamples into two halves and used one half for split selection and the other for within-leaf estimation. Though sampling splitting is sometimes criticized as inefficient as they do not use the full training data at each estimation steps; however, they argued that by aggregating over multiple trees, each sample will be used for split selection or leaf estimation. By doing so, therefore, we can efficiently achieve honesty criterion without wasting half of the samples.  

Wager and Athey’s ideas are similar to sample splitting and cross fitting in double machine learning. To perform the sample splitting to control for bias, we need to first split data into main sample and auxiliary sample, and then use the main sample to train and the auxiliary sample to estimate. Next, we swap the role of main sample and auxiliary sample to re-estimate. Finally, we combine the results by averaging over the two estimators. By doing so, we can achieve a good estimator without losing efficiency. The idea of sample splitting is an analogy to $K$-fold cross validation and can be also generalized to $K$-fold splitting and cross-fitting. For $K$-fold cases, we take a $K$-fold random partition of data rather than splitting into half, and at the end we aggregate over $K$ machine learning estimators so that we use the full data. In terms of the choice of $K$, Chernozhukov et al. (2018) recommends using moderate $K$ values such as 4 or 5.

## 3. Set up of Double Machine Learning with Partial Linear Model

Consider the partial linear model (using the notations in Chernozhukov et al. (2018):
$$Y=D\theta_0+g_0 (x)+U,\qquad E[U|X,D]=0,$$
$$D=m_0(X)+V,\qquad E[V|X]=0,$$
where $Y$ is the outcome variable, $D$ is the treatment indicator, $X$ is a vector of confounders, $U$ and $V$ are disturbances, and $\theta_0$ is the causal estimand we would like to infer.  

Next, consider the double machine learning procedure with sample splitting, following Chernozhukov et al. (2018):  
- Take a $K$-fold random partition $(I_k)^{K}_{k=1}$ of observation indices $[N]=\{1,\cdots,N\}$ such that the size of each fold $I_k$ is $N/K.$ And define $I^c_{k}=\{1,\cdots,N\}$ ``\`` $I_k$ for each $k$.  
- For $k=1$, using machine learning methods to estimate $\hat{g_0}$ and $\hat{m_0}$ using samples $I^c_{k=1}.$  
- Predict $Y$ and $D$ using $X$ by $\widehat{\mathbb{E}[Y|X])}$ and $\widehat{\mathbb{E}[D|X])}$ respectively on $I_{k=1}$.  
- Get residuals $\hat{U}=Y-\widehat{\mathbb{E}[Y|X])}$ and $\hat{V}=D-\widehat{\mathbb{E}[D|X])}$  
- Regress $\hat{U}$ on $\hat{V}$ to get the estimate $\widehat{\theta_{0,k=1}}$ using samples $I_{k=1}.$  
- Repeat, obtain the $K$ estimators for $k=1,\cdots,K,$ average them, and get $\hat{\theta_0}=\frac{1}{K}\sum^{K}_{k=1}\widehat{\theta_{0,k}}$  

Following the procedures, the estimator $\hat{\theta_0}$ will be a consistent and asymptotic normal estimator. In other words, the distribution of $\hat{\theta_0}-\theta_0$ will follow a normal distribution and center around zero.

## 4. Replicate Chernozhukov et al. (2018)

We found the codes for generating the Figure 2 in Chernozhukov et al. (2018) on GitHub. The purpose of the figures is to compare full sample with sample splitting and cross fitting procedures. We replicated and rewrote the codes in $R$ and generated a similar plot. 

```{r,echo=F,warning=F}
ggdraw() + 
  draw_image("Fig2_full.png", width = 0.5) + 
  draw_image("Fig2_split.png", width = 0.5, x = 0.5) 
```

To illustrate that sample splitting can combat overfitting bias, Chernozhukov et al. (2018) uses an artificial example. Let $X\sim N(0,1)$ and fix true $\theta_0$ be 1. And the data is generated by $Y=D+X+U$ and $D=X+V$, where $U$ and $V$ are error terms and follow $N\sim(0,1)$. The overfit is specified as $(y_i-x_i)/N^{1/2-\epsilon}$. More specifically, $\hat{x_i}=x_i+(y_i-x_i)/N^{1/2-ϵ}.$ Then two estimators $\hat{\theta_0}$ are calculated, one using the full sample, and the other adopting two-fold sample splitting and cross fitting. The double machine learning estimator is formulated as
$$\hat{\theta_0}=\left(\frac{1}{n}\sum_{i\in I}\hat{V_i}D_i\right)^{-1}\frac{1}{n}\sum_{i\in I}\hat{V_i}(Y_i-\hat{g_0}(X_i)).$$
Note that, when we use two-fold sample splitting and cross-fitting and if we partition our data into $I$ and $I^C$, then $\hat{\theta_0}$ for $I$ is calculated using the $\hat{g_0}$ estimated with observations in $I^C$, and vice versa. And we simply average over $\hat{\theta_0}$ for $I$ and $\hat{\theta_0}$ for $I^C$ to get our causal estimand using the full data. For the above plot, we fix $\epsilon=0.1$. The histograms represent the distribution of the studentized $\hat{\theta_0}$ calculated as $\frac{\hat{\theta_0}-\theta_0}{s.e.(\hat{\theta_0})},$ which can reflect the bias. And the red curve is the density for $N\sim(0,1).$ From the plots, we may observe that using the full sample and without sample splitting (left) clearly induce large bias as the distribution is shifted to the left and not centered around 0; however sample splitting with cross fitting (right) can remove the bias very well.

## 5. Simulation Study Setup

We explored 128 different scenarios in the data generation process with following configurations.  
- Number of covariates: 5 or 100  
- Sample size $N$: 50 or 1000  
- Number of folds $K$: 2, 5  
- Variance of the error term $U$ and $V$: 0.1 or 1  
- Distribution of coefficients for covariates $g_0$: $N\sim(1,0.5^2)$ or $U\sim(0.5,1.5)$  
- Outcome model: linear or non-linear with sine functions

We fixed the true causal effect $\theta_0$ be 1. And the machine learning methods we considered are LASSO and Extreme Gradient Boosting (XGBoost). We fit LASSO using `glmnet` package and the value of $\lambda$ for LASSO is determined by 5-fold cross validation, where we compared $\lambda$ from 0.001 to 100. We fit XGBoost using `xgboost` package. We did not tune XGBoost because we found the loss is small using the parameters provided in the documentation. For each scenario, we generated 100 different data sets and conduct double machine learning using the partial linear models with sample splitting and without sample splitting. The codes for simulations and the simulation results are provided.

## 6. Simulation Results

We first made a plot of 128 mean differences in two studentized estimators from using the full sample and using the sample splitting correspond to 128 different scenarios.

![](report_image/hist_diffs.png){width=250px}

We may observe that most of the differences are around zero, suggesting that the difference between using sample splitting or not is small under most scenarios. However, we can still notice that under some scenarios the difference is huge, especially those on the left tail, suggesting that sample splitting may sometimes lead to larger bias. The conclusion is surprising, and we decided to look into the scenarios on the tail regions. 

![Scenario1](report_image/1.png){width=250px} ![Scenario2](report_image/2.png){width=250px}

The above two plots are scenario 1 and 2, where the machine learning method is XGBoost, number of covariates is 5, sample size is 50, coefficients for covariates follow normal distribution, and the outcome model is generated from linear functions. The left plot is 2-fold sample splitting and the right plot is 5-fold. Under these two scenarios, we can see that using the full sample may cause large bias, and sample splitting can reduce the bias.

## 7. Real Datasets

We applied the methods described in last section to a real dataset. It is the Right Heart Catheterization (RHC) dataset we used in Homework 3. The data is from Murphy and Cluff (1990), then re-analyzed by Connors et al. (1996) in an influential study. The data contains 5735 samples and 51 covariates. The outcome variable is dth30, which is the vital status at 30 days after admission. And the treatment indicator is TRUE if RHC was applied within 24 hours of admission, and FALSE if RHC was not applied within 24 hours of admission. We are interested in estimating the average treatment effect. 

```{r,echo=FALSE,warning=F,message=F,eval=F}
source("real_data.R")
round_df <- function(x, digits) {
    numeric_columns <- sapply(x, mode) == 'numeric'
    x[numeric_columns] <-  round(x[numeric_columns], digits)
    x
}

real_res = real_data_res()
real_res = round_df(real_res, 4)
```
```{r,echo=F,eval=F}
real_res %>% flextable() %>% autofit() %>% font(fontname = "Cambria",part = "all")
```


## 8. Conclusions

## References

- Athey, Susan, and Guido Imbens. “Recursive Partitioning for Heterogeneous Causal Effects.” Proceedings of the National Academy of Sciences, vol. 113, no. 27, 2016, pp. 7353–7360., https://doi.org/10.1073/pnas.1510489113. 
- Chernozhukov, Victor, et al. “Double/Debiased Machine Learning for Treatment and Structural Parameters.” The Econometrics Journal, vol. 21, no. 1, 2018, https://doi.org/10.1111/ectj.12097. 
- Wager, Stefan, and Susan Athey. “Estimation and Inference of Heterogeneous Treatment Effects Using Random Forests.” Journal of the American Statistical Association, vol. 113, no. 523, 2018, pp. 1228–1242., https://doi.org/10.1080/01621459.2017.1319839. 
- Seo, Michael, et al. “Comparing Methods for Estimating Patient‐Specific Treatment Effects in Individual Patient Data Meta‐Analysis.” Statistics in Medicine, vol. 40, no. 6, 2020, pp. 1553–1573., https://doi.org/10.1002/sim.8859. 
- Codes for figures in Section 4: https://github.com/VC2015/DMLonGitHub/blob/master/Figure2.m 
- Lecture Notes provided by Professor Fan Li (Assessed via Sakai) 
- Chernozhukov, Victor DML Slides, “Double/Debiased Machine Learning for Causal and Treatment Effects,” May 31, 2018. (Accessed via Sakai) 
- Datasets provided by Professor Fan Li (Assessed via Sakai)

