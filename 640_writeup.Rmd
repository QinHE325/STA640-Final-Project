---
title: "Does sample splitting help when estimating causal effects using double machine learning methods?"
author: "TJ Tang, Qin He"
date: "`r Sys.Date()`"
output: word_document
---

```{r setup, include=FALSE}
library(cowplot)
library(magick)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(dpi = 300) 
```

## 1. Introduction

Sample splitting is one of the key issues when adapting machine learning methods to causal inference problems. However, there are some anecdotes saying that sample splitting will not help in finite samples. Therefore, this paper study will first go through Chernozhukov et al. (2018) especially about an example that sample splitting reduces the bias, and then will conduct a few simulation studies to figure out the effect of sample splitting when estimating causal effects.

## 2. Double Machine Learning and Sample Splitting

Chernozhukov et al. (2018) establish that in very high dimension cases, machine learning methods will employ regularization to reduce variance but cause the regularization bias and overfitting on the parameters of interest or causal treatment effect parameter, for example average treatment effect. They show that the bias and overfit can be removed by double machine learning methods with sample splitting and cross fitting. Applying double machine learning methods can overcome the regularization bias because it employs an orthogonalized formulation. Meanwhile, sample splitting and cross fitting can reduce the bias due to the overfitting because we can avoid using the same data twice, to train the estimator and predict the missing potential outcomes. The details for orthogonalization and why double machine learning methods reduce the regularization bias is not the focus of this project. Instead, this project will focus more on the sample splitting and the effect to reduce overfitting bias.  

Wager and Athey (2018) illustrated double-sample trees as a motivation to reduce bias. They divided training subsamples into two halves and used one half for split selection and the other for within-leaf estimation. Though sampling splitting is sometimes criticized as inefficient as they do not use the full training data at each estimation steps; however, they argued that by aggregating over multiple trees, each sample will be used for split selection or leaf estimation. By doing so, therefore, we can efficiently achieve honesty criterion without wasting half of the samples.  

Wager and Athey’s ideas are similar to sample splitting and cross fitting in double machine learning. To perform the sample splitting to control for bias, we need to first split data into main sample and auxiliary sample, and then use the main sample to train the auxiliary sample to estimate. Next, we swap the role of main sample and auxiliary sample to estimate. Finally, we combine the results by averaging over the two estimators. By doing so, we can achieve a good estimator without losing efficiency. The idea of sample splitting is an analogy to $K$-fold cross validation and can be also generalized to $K$-fold splitting and cross-fitting. For $K$-fold cases, we take a $K$-fold random partition of data rather than splitting into half, and at the end we aggregate over $K$ machine learning estimators so that we use the full data. In terms of the choice of $K$, Chernozhukov et al. (2018) recommends using moderate $K$ values such as 4 or 5.

## 3. Set up of Double Machine Learning with Partial Linear Model

Consider the partial linear model (using the notations in Chernozhukov et al. (2018):
$$Y=D\theta_0+g_0 (x)+U,\qquad E[U|X,D]=0,$$
$$D=m_0(X)+V,\qquad E[V|X]=0,$$
where $Y$ is the outcome variable, $D$ is the treatment indicator, $X$ is a vector of confounders, $U$ and $V$ are disturbances, and $\theta_0$ is the causal estimand we would like to infer.  

Next, consider the double machine learning procedure with sample splitting, following Chernozhukov et al. (2018):  
- Take a $K$-fold random partition $(I_k)^{K}_{k=1}$ of observation indices $[N]=\{1,\cdots,N\}$ such that the size of each fold $I_k$ is $N/K.$ And define $I^c_{k}=\{1,\cdots,N\}$ ``\`` $I_k$ for each $k$. 
- Using machine learning methods to predict $Y$ and $D$ using X by $\widehat{\mathbb{E}[Y|X])}$ and $\widehat{\mathbb{E}[D|X])}$ respectively.  
- Get residuals $\hat{U}=Y-\widehat{\mathbb{E}[Y|X])}$ and $\hat{V}=D-\widehat{\mathbb{E}[D|X])}$  
- Regress $\hat{U}$ on $\hat{V}$ to get the estimate $\hat{\theta_0}$  

Following the procedures, the estimator $\hat{\theta_0}$ is a consistent estimator and asymptotic normal estimator.

## 4. Replicate Chernozhukov et al. (2018)

We found the codes for generating the Figure 2 in Chernozhukov et al. (2018) on GitHub. We replicated and rewrote the codes in $R$ and generated a similar plot. 

```{r,echo=F,warning=F}
ggdraw() + 
  draw_image("Fig2_full.png", width = 0.5) + 
  draw_image("Fig2_split.png", width = 0.5, x = 0.5) 
```

To illustrate that sample splitting can combat overfitting bias, Chernozhukov et al. (2018) uses an artificial example. Let $X\sim N(0,1)$ and fix true $\theta_0$ be 1. And the data is generated by $Y=D+X+U$ and $D=X+V$, where $U$ and $V$ are error terms and follow $N\sim(0,1)$. The overfit is specified as $(y_i-x_i)/N^{1/2-\epsilon}$. More specifically, $\hat{x_i}=x_i+(y_i-x_i)/N^{1/2-ϵ}.$ Then two estimators $\hat{\theta_0}$ are calculated, one using the full sample, and the other adopting two-fold sample splitting and cross fitting. The double machine learning estimator is formulated as
$$\hat{\theta_0}=\left(\frac{1}{n}\sum_{i\in I}\hat{V_i}D_i\right)^{-1}\frac{1}{n}\sum_{i\in I}\hat{V_i}(Y_i-\hat{g_0}(X_i)).$$
Note that, when we use two-fold sample splitting and cross-fitting and if we partition our data into $I$ and $I^C$, then $\hat{\theta_0}$ for $I$ is calculated using the $\hat{g_0}$ estimated with observations in $I^C$, and vice versa. And we simply average over $\hat{\theta_0}$ for $I$ and $\hat{\theta_0}$ for $I^C$ to get our causal estimand using the full data. For the above plot, we fix $\epsilon=0.1$. The histograms represent the distribution of the studentized $\hat{\theta_0}$ calculated as $\frac{\hat{\theta_0}-\theta_0}{s.e.(\hat{\theta_0})},$ which can reflect the bias. And the red curve is the density for $N\sim(0,1).$ From the plots, we may observe that using the full sample and without sample splitting (left) clearly induce large bias as the distribution is shifted to the left and not centered around 0; however sample splitting with cross fitting (right) can remove the bias very well.

## 5. Simulation Study

We explored 128 different scenarios in the data generation process with following configurations.  
- Number of covariates: 5 or 100  
- Sample size $N$: 50 or 1000  
- Number of folds $K$: 2, 5  
- Variance of the error term $U$ and $V$: 0.1 or 1  
- Distribution of coefficients for covariates $g_0$: $N\sim(1,0.5^2)$ or $U\sim(0.5,1.5)$  
- Outcome model: linear or non-linear with sine functions

We fixed the true causal effect $\theta_0$ be 1. And the machine learning methods we considered are LASSO and Extreme Gradient Boosting (XGBoost). We fit LASSO using `glmnet` package and the value of $\lambda$ for LASSO is determined by 5-fold cross validation, where we compared $\lambda$ from 0.001 to 100. We fit XGBoost using `xgboost` package. We did not tune XGBoost because we found the loss is small using the parameters provided in the documentation. For each scenario, we generated 100 different data sets and conduct double machine learning using the partial linear models with sample splitting and without sample splitting. The codes for simulations and the simulation results are provided.

## 6. Real Datasets

We applied the methods described in last section to a real dataset. It is the Right Heart Catheterization (RHC) dataset we used in Homework 3. The data is from Murphy and Cluff (1990), then re-analyzed by Connors et al. (1996) in an influential study. The data contains 5735 samples and 51 covariates. The outcome variable is dth30, which is the vital status at 30 days after admission. And the treatment indicator is TRUE if RHC was applied within 24 hours of admission, and FALSE if RHC was not applied within 24 hours of admission. We are interested in estimating the average treatment effect. 

## 7. Conclusions

## References

- Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, James Robins, Double/debiased machine learning for treatment and structural parameters, The Econometrics Journal, Volume 21, Issue 1, 1 February 2018, Pages C1–C68, https://doi.org/10.1111/ectj.12097  
- Stefan Wager & Susan Athey (2018) Estimation and Inference of Heterogeneous Treatment Effects using Random Forests, Journal of the American Statistical Association, 113:523, 1228-1242, DOI: 10.1080/01621459.2017.1319839  
- Victor Chernozhukov DML Slides, “Double/Debiased Machine Learning for Causal and Treatment Effects,” May 31, 2018. (Accessed via Sakai)  
- Seo M, White IR, Furukawa TA, et al. Comparing methods for estimating patient-specific treatment effects in individual patient data meta-analysis. Statistics in Medicine. 2021;40:1553–1573. https://doi.org/10.1002/sim.8859  
- Codes for figures in Section 4: https://github.com/VC2015/DMLonGitHub/blob/master/Figure2.m  
- Lecture Notes provided by Professor Fan Li (Assessed via Sakai)  
- Datasets provided by Professor Fan Li (Assessed via Sakai)

