---
title: "real_data"
author: "TJ Tang"
date: "4/24/2022"
output: pdf_document
---

```{r setup, include=FALSE}
library(writexl)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
df_to_excel = function(fs,ss,name){
  df = data.frame(
    full_sample = fs,
    sample_split = ss
  )
  path = paste0("data/",name,".csv")
  write_xlsx(df, path)
}
```

## 0 Read data

```{r}
rhc <- read.csv("rhc.xls",stringsAsFactors = TRUE)
rhc$treatment = as.numeric(rhc$treatment)
rhc$dth30 = as.numeric(rhc$dth30)
rhc <- rhc %>% dplyr::select(-ID)
rhc <- rhc %>% rename("D"=treatment, "Y"=dth30)
head(rhc)
```


## 1 sample splitting

```{r}
sample_splitting <- function(df,k = 2, ml_method = "xgboost", seed=123){
  set.seed(seed)
  ## partial linear model
  folds <- createFolds(df$Y, k=k)
  ate <- c()
  for (i in 1:k){
    # split into training and prediction sample
    train <- df[-unlist(folds[i]),]
    prediction <- df[unlist(folds[i]),] 
    # lasso
    if(ml_method == "lasso"){
      lambdas <- 10^seq(2, -3, by = -.1)
      x = model.matrix(dth30 ~., train)[,-1]
      d = model.matrix(treatment ~., train)[,-1]
      y = train$dth30
      D = train$treatment
      newx_y = model.matrix(dth30 ~., prediction)[,-1]
      newx_d = model.matrix(treatment ~., prediction)[,-1]
      
      lasso_reg_y <- cv.glmnet(x, y, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 5)
      fit_y = glmnet(x, y, data = train, lambda= lasso_reg_y$lambda.min , family='gaussian', intercept = F, alpha=1)
      lasso_reg_z <- cv.glmnet(d, D, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 5)
      fit_z = glmnet(d, D, data = train, lambda= lasso_reg_z$lambda.min, family='gaussian', intercept = F, alpha=1)
      
      # predict Y and Z
      py <- predict(fit_y, newx_y)
      pz <- predict(fit_z, newx_d)
      
      # get residuals
      u <- prediction$Y - py
      v <- prediction$D - pz
    }
    
    # xgboost
    if(ml_method == "xgboost"){
      
      dtrain = as.matrix(train)
      
      fit_y <- xgboost(data = dtrain, label = train$Y, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "reg:squarederror", verbose = FALSE)
      fit_z <- xgboost(data = dtrain, label = train$D, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic", verbose = FALSE)
      
      # predict Y and Z
      py <- predict(fit_y, as.matrix(prediction))
      pz <- predict(fit_z, as.matrix(prediction))
      
      # get residuals
      u <- prediction$Y - py
      v <- prediction$D - pz
    }
    
    # get the estimate of theta
    m1 <- lm(u~v)
    ate <- c(ate,m1$coefficients[2])
  }
  fit.ite <- mean(ate) # mean ate of K folds
  return(fit.ite) # ite equals ate
}
```


```{r}

```


## 2 Without sample splitting

```{r}
full_sample <- function(df, ml_method = "xgboost", seed=123){
  set.seed(seed)
  # lasso
  if(ml_method == "lasso"){
    lambdas <- 10^seq(2, -3, by = -.1)
    x = model.matrix(Y ~., df)[,-1]
    d = model.matrix(D ~., df)[,-1]
    y = df$Y
    D = df$D
    newx_y = model.matrix(Y ~., df)[,-1]
    newx_d = model.matrix(D ~., df)[,-1]
    
    lasso_reg_y <- cv.glmnet(x, y, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 5)
    fit_y = glmnet(x, y, data = train, lambda= lasso_reg_y$lambda.min , family='gaussian', intercept = F, alpha=1)
    lasso_reg_z <- cv.glmnet(d, D, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 5)
    fit_z = glmnet(d, D, data = train, lambda= lasso_reg_z$lambda.min, family='gaussian', intercept = F, alpha=1)
    
    # predict Y and Z
    py <- predict(fit_y, newx_y)
    pz <- predict(fit_z, newx_d)
    
    # get residuals
    u <- df$Y - py
    v <- df$D - pz
  }
  
  # xgboost
  if(ml_method == "xgboost"){
    ddf = as.matrix(df)
    
    fit_y <- xgboost(data = ddf, label = df$Y, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "reg:squarederror", verbose = FALSE)
    fit_z <- xgboost(data = ddf, label = df$D, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic", verbose = FALSE)
    
    # predict Y and Z
    py <- predict(fit_y, ddf)
    pz <- predict(fit_z, ddf)
    
    # get residuals
    u <- df$Y - py
    v <- df$D - pz
    
  }
  
  
  
  # get the estimate of theta
  m1 <- lm(u~v)
  return(m1$coefficients[2]) # ite equals ate
}
```

