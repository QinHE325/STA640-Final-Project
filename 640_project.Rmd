---
title: "640_project"
author: "TJ Tang"
date: "4/20/2022"
output: html_document
---

```{r setup, include=FALSE}
library(randomForest)
require(tidyverse)
require(caret)
require(ggplot2)
require(ranger)
require(patchwork)
require(e1071)
require(glmnet)
require(xgboost)
set.seed(0224)
knitr::opts_chunk$set(echo = TRUE)
```

## DML

Consider the following partially linear regression (PLR) model as in Robinson (1988):

$$Y=D\theta_0+g_0(X)+U$$
$$D=m_0(X)+V$$

## replicate results

$$Y=X+Z+V, V\sim N(0,1)$$
$$X=Z+U, U\sim N(0,1)$$
overfit = (y-z)/(n**(1/2.5))
Z = g_0(X_i)

$$\hat{g_0}(X_i ) = g_0(X_i ) + (Y_i − g_0(X_i))/N^{1/2−\epsilon}$$

```{r}
generate_data <- function(ncov=5,n=1000,theta0=0.5, 
                          p = 0.5, mean = 0, variance = 1, 
                          g0_dist = "normal", 
                          outcome_model = "linear",  
                          params = c(1,0.5), seed = 640) {
  set.seed(seed)
  # simulate data
  X = matrix(NA,ncol=ncov,nrow=n)
  for(i in 1:ncov){
    X[,i] <-rnorm(n)
  }
  
  X <- apply(X,2,scale)
  D <- rbinom(n,1,p)
  U <- rnorm(n, mean, variance)
  V <- rnorm(n, mean, variance)
  
  if(g0_dist == "normal"){
    g0 <- rnorm(ncov,params[1],params[2])
  }else if(g0_dist == "uniform"){
    g0 <- runif(ncov,0.5,1.5)
  }
  
  data <- model.matrix(~X+D-1)

  if(outcome_model == "linear"){ 
    Y <- as.vector(data%*%c(g0,theta0))+U
  }else{ # model non-linearity
    g1 <- rnorm(ncov,params[1],params[2])
    # add sine elements
    Y <- as.vector(data%*%c(g0,theta0)) + 
      as.vector(sin(data)%*%c(g1,theta0))
  }
  df <- data.frame(cbind(Y,data))
  return(df)
}

```



```{r,warning=F}
sample_splitting <- function(df,k = 2, ml_method = "random forest", seed=123){
  set.seed(seed)
  ## partial linear model
  folds <- createFolds(df$Y, k=k)
  ate <- c()
  for (i in 1:k){
    # split into training and prediction sample
    train <- df[-unlist(folds[i]),]
    prediction <- df[unlist(folds[i]),] 
    
    # random forests
    if(ml_method == "random forest"){
      fit_y <- ranger(Y ~ .-D,data=train)
      fit_z <- ranger(D ~ .-Y,data=train)
       # predict Y and Z
      py <- predict(fit_y, prediction)
      pz <- predict(fit_z, prediction)
      
      # get residuals
      u <- prediction$Y - py$predictions
      v <- prediction$D - pz$predictions
    }
    
    # svm
    if(ml_method == "svm"){
      fit_y = svm(Y ~ ., data = train, kernel = "linear", cost = 10, scale = FALSE)
      fit_z = svm(D ~ ., data = train, kernel = "linear", cost = 10, scale = FALSE)
       # predict Y and Z
      py <- predict(fit_y, prediction)
      pz <- predict(fit_z, prediction)
      
      # get residuals
      u <- prediction$Y - py
      v <- prediction$D - pz
    }
    
    # lasso
    if(ml_method == "lasso"){
       x = model.matrix(Y ~., train)[,-1]
       d = model.matrix(D ~., train)[,-1]
       y = train$Y
       D = train$D
       newx_y = model.matrix(Y ~., prediction)[,-1]
       newx_d = model.matrix(D ~., prediction)[,-1]
       
       fit_y = glmnet(x, y, data = train, lambda= 0.5, family='gaussian', intercept = F, alpha=1)
       fit_z = glmnet(d, D, data = train, lambda= 0.5, family='gaussian', intercept = F, alpha=1)
        # predict Y and Z
       py <- predict(fit_y, newx_y)
       pz <- predict(fit_z, newx_d)
      
       # get residuals
       u <- prediction$Y - py
       v <- prediction$D - pz
    }
    
    # xgboost
    if(ml_method == "xgboost"){
      dtrain = as.matrix(train)
      
      fit_y <- xgboost(data = dtrain, label = train$Y, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "reg:squarederror", verbose = FALSE)
      fit_z <- xgboost(data = dtrain, label = train$D, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic", verbose = FALSE)
      
      # predict Y and Z
      py <- predict(fit_y, as.matrix(prediction))
      pz <- predict(fit_z, as.matrix(prediction))
      
      # get residuals
      u <- prediction$Y - py
      v <- prediction$D - pz
      
      
    }
    
    
    # get the estimate of theta
    m1 <- lm(u~v)
    ate <- c(ate,m1$coefficients[2])
  }
  fit.ite <- mean(ate) # mean ate of 5 folds
  return(fit.ite) # ite equals ate
}
```



```{r,warning=F}
full_sample <- function(df, ml_method = "random forest", seed=123){
  set.seed(seed)
  ## partial linear model
  # random forests
  
  # random forests
  if(ml_method == "random forest"){
      fit_y <- ranger(Y ~ .-D,data=df)
      fit_z <- ranger(D ~ .-Y,data=df)
      # predict Y and Z
      py <- predict(fit_y, df)
      pz <- predict(fit_z, df)
      # get residuals
      u <- df$Y - py$predictions
      v <- df$D - pz$predictions
  }
  
   # svm
  if(ml_method == "svm"){
      fit_y = svm(Y ~ ., data = df, kernel = "linear", cost = 10, scale = FALSE)
      fit_z = svm(D ~ ., data = df, kernel = "linear", cost = 10, scale = FALSE)
      # predict Y and Z
      py <- predict(fit_y, df)
      pz <- predict(fit_z, df)
      # get residuals
      u <- df$Y - py
      v <- df$D - pz
  }
  
  # lasso
  if(ml_method == "lasso"){
       x = model.matrix(Y ~., df)[,-1]
       d = model.matrix(D ~., df)[,-1]
       y = df$Y
       D = df$D
       newx_y = model.matrix(Y ~., df)[,-1]
       newx_d = model.matrix(D ~., df)[,-1]

       fit_y = glmnet(x, y, lambda= 0.5, family='gaussian', intercept = F, alpha=1)
       fit_z = glmnet(d, D, lambda= 0.5, family='gaussian', intercept = F, alpha=1)
        # predict Y and Z
       py <- predict(fit_y, newx_y)
       pz <- predict(fit_z, newx_d)
      
       # get residuals
       u <- df$Y - py
       v <- df$D - pz
  }
  
  # xgboost
  if(ml_method == "xgboost"){
      ddf = as.matrix(df)
      
      fit_y <- xgboost(data = ddf, label = df$Y, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "reg:squarederror", verbose = FALSE)
      fit_z <- xgboost(data = ddf, label = df$D, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic", verbose = FALSE)
      
      # predict Y and Z
      py <- predict(fit_y, ddf)
      pz <- predict(fit_z, ddf)
      
      # get residuals
      u <- df$Y - py
      v <- df$D - pz
      
      
  }
    


  # get the estimate of theta
  m1 <- lm(u~v)
  return(m1$coefficients[2]) # ite equals ate
}
```

```{r,warning=F}
theta0 <- 0.5
df1 <- generate_data(n=1000,theta0=theta0)

ss <- fs <- c()
for(i in 1:100){
  df1 <- generate_data(n=1000,theta0=theta0, seed = i)
  ss <- c(ss,sample_splitting(df1,seed=640,  ml_method = "xgboost"))
  fs <- c(fs,full_sample(df1,seed=640,  ml_method = 'xgboost'))
}


```


```{r,warning=F}
p1 <- ggplot(mapping=aes((ss-theta0))) + geom_histogram(bins=30) 
p2 <- ggplot(mapping=aes((fs-theta0))) + 
  geom_histogram(bins=30) +
  xlim(ggplot_build(p1)$layout$panel_scales_x[[1]]$range$range) 
p1+p2
```
